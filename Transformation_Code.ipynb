{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import h5py \n",
    "import csv \n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_loc = 'iel.hdf5'                   #Location of the hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "def fill_null(dset,time_index):\n",
    "    data = dset.copy()\n",
    "    time = data.loc[:,time_index]\n",
    "    tlist = time.tolist()\n",
    "    start = dt.datetime(year = min(tlist).year, month = 1, day = 1, hour = 0, minute = 0)\n",
    "    end = dt.datetime(year = max(tlist).year, month = 12, day = 31, hour = 23, minute = 0)\n",
    "    i = start\n",
    "    while i<=end:\n",
    "        if i not in tlist:\n",
    "            data = data.append([(i,np.nan)])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color =#bf5700>UT Austin Dataset</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Data Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the data files is segregated by building and then again by year. For example, 2015 data for the Welch building is located in 'utexas/WEL/2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = 'utexas'                         #Insert home directory where data is stored\n",
    "ext = '.csv'                            #Insert data storage type\n",
    "separ = ','                             #Separation\n",
    "mdata = 'utexas/utexas_metadata.csv'    #Point to location of metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root, folders,x = [m for m in os.walk(home)][0]\n",
    "folder_paths = [os.path.join(home, subdir) for subdir in folders]\n",
    "subfiles = []\n",
    "for f in folder_paths: \n",
    "    r,a, data = [m for m in os.walk(f)][0]\n",
    "    for d in data: \n",
    "        if os.path.splitext(d)[1]==ext:\n",
    "            subfiles.append(os.path.join(r,d))\n",
    "hdf_dsets = [s.replace('\\\\','/').replace(ext,'') for s in subfiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Subfiles' should now have a list of the complete path of all data files\n",
    "Ut dataset involves various time formats. convert_any_time converts string formats to pd datetime while float formats (already in pd format) are left alone: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_any_dtime(x):\n",
    "    try:\n",
    "        return dt.datetime.strptime(x, \"%m/%d/%y %H:%M\")\n",
    "    except:\n",
    "        try:\n",
    "            return dt.datetime.strptime(x, \"%m/%d/%Y %H:%M\")\n",
    "        except:\n",
    "            return None\n",
    "def read_mdata(mdata_file):\n",
    "    return pd.read_csv(mdata_file,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_db = read_mdata(mdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that compiles the data into the file. Be warned, data processing may take over 20 minutes. For a sense of where in the process the computer is, remove the commented portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_loc,'a') as f:\n",
    "    #counter = 0\n",
    "    for data_file in subfiles: \n",
    "        #counter = counter+1\n",
    "        #print('Now reading dataset #{}......................{}'.format(counter,data_file))\n",
    "        d = pd.read_csv(data_file).dropna(axis=0,how='any',inplace=False)\n",
    "        dt_dates = [convert_any_dtime(x) if type(x)==str else x for x in d['DateTime'] ]\n",
    "        dates= np.array( [x.strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8') for x in dt_dates], dtype = np.string_).reshape((-1,1))\n",
    "        usage =np.array([x for x in d['Electrical ( kWh )']],dtype=np.float64).reshape((-1,1))\n",
    "        hdfs_dset = data_file.replace('\\\\','/').replace(ext,'')\n",
    "        f[hdfs_dset] = np.hstack((dates,usage))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Metadata Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_loc,'a') as f:\n",
    "    for building in meta_db.index:\n",
    "        psu,sqft = meta_db.loc[building]\n",
    "        if '{}/{}'.format(home,building) in f:\n",
    "            bldg_grp = f['{}/{}'.format(home,building)]\n",
    "            bldg_grp.attrs['PSU'] = psu\n",
    "            bldg_grp.attrs['Sqft'] = sqft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>EUI</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eui(dset,axis,sqft):\n",
    "    hours = len(dset)\n",
    "    energy = dset.loc[:,axis].tolist()\n",
    "    #If there are missing values, scale EUI(in kWH/sqft) linearly*\n",
    "    total_e = sum(energy)*8766/hours\n",
    "    #Convert total energy (kWH) to (kbTU)\n",
    "    total_e = total_e *3.412141633\n",
    "    return total_e/sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_loc,'a') as f:\n",
    "    subset = f[home]\n",
    "    for buil in subset.keys():\n",
    "        building = subset[buil]\n",
    "        if 'Sqft' in building.attrs:\n",
    "            sqft = building.attrs['Sqft']\n",
    "            for year in building.keys():\n",
    "                dset = pd.DataFrame(building[year][()])\n",
    "                building[year].attrs['EUI'] = get_eui(dset,1,sqft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><font color = #ff1111>---TODO--- decide whether to fill in incomplete data using regression to calculate EUI or scale linearly with sum(available data)*(8760 hrs/yr)/(hours available data) </font></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color =#bf5700>MIT Dataset</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = 'MIT'                            #Insert home directory where data is stored\n",
    "ext = '.csv'                            #Insert data storage type\n",
    "separ = ','                             #Separation\n",
    "mdata = 'mdata/mdata.csv'               #Point to location of metadata file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Data Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2014.csv', '2015.csv', '2016.csv']\n"
     ]
    }
   ],
   "source": [
    "root, folders,files = [m for m in os.walk(home)][0]\n",
    "subfiles = []\n",
    "for d in files: \n",
    "    if os.path.splitext(d)[1]==ext:\n",
    "        subfiles.append(os.path.join(root,d))\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIT dataset is shaped differently than our desires in a few ways: \n",
    "<ul>\n",
    "<li>The files are segregated by year instead of by building</li>\n",
    "<li>The files contain multiple 'error' values, so we need to screen more than just N/A</li>\n",
    "<li>The list is NOT ordered, meaning that time intervals next to each other in the original scrape will NOT necessarily be together in the final </li>\n",
    "<li>In addition the separation between timestamps is not hourly as we would like but every 15 minutes</li>\n",
    "<li> To further complicate matters, the list increments are in kilowatts (kW), a unit of power, not a unit of energy!!!  Power is related to energy usage in that power is average energy usage over a period of time. A Watt is 1 Joule per Second, and so 1 <b>kilowatt hour</b> is our standard measurement of energy: as\n",
    "$$1\\text{ kWH} =\\frac{\\text{1000 Joule/sec}}{\\text{1 kW}}\\times \\frac{\\text{3600 seconds}}{1 hr}= 3600000\\text{ Joule}$$\n",
    "A 1 kW power usage for 15 minutes is therefore $\\frac{1}{4} \\times 3600000 \\text{ Joule} = 900000 \\text{Joule}$\n",
    "Thus to find the total power usage over 4 different 15 minute intervals, take the average of the power usage of each individual file \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mit_name_clean(colname):\n",
    "    colname = colname.replace('RealPower','')\n",
    "    return colname.replace('TFR','')\n",
    "def mit_aggregate(sorted_array,name):\n",
    "    sorted_array.drop_duplicates(subset = 'DATE_TIME', keep = 'first', inplace = True )\n",
    "    times = sorted_array['DATE_TIME'].tolist()\n",
    "    usage = sorted_array[name].tolist()\n",
    "    aggregated_times = []\n",
    "    aggregated_usage = []\n",
    "    for i in range(len(times)-3):\n",
    "        one = (times[i+1]-times[i] ==dt.timedelta(minutes=15))\n",
    "        two = (times[i+2]-times[i+1] ==dt.timedelta(minutes=15))\n",
    "        three = (times[i+3]-times[i+2] ==dt.timedelta(minutes=15))\n",
    "        if(times[i].minute==0 and one and two and three):\n",
    "            aggregated_times.append(times[i].strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8'))\n",
    "            aggregated_usage.append(str((usage[i]+usage[i+1]+usage[i+2]+usage[i+3])/4).encode('utf8'))\n",
    "    return np.hstack((np.array(aggregated_times).reshape((-1,1)),np.array(aggregated_usage).reshape((-1,1))))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can account by the error values by setting up a filter, and deal with the time issue by 1) sorting the dataframe before partitioning it and 2) writing the aggregate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ERROR_VALUES = ['No Data','I/O Timeout','Error','Pt Created','Configure']\n",
    "with h5py.File(file_loc) as f:\n",
    "    grp = f[home] if home in f else f.create_group(home)\n",
    "    for dfile in subfiles: \n",
    "        year = dfile.split('\\\\')[-1].split('.')[0]\n",
    "        df = pd.read_csv(dfile)\n",
    "        print('Now analyzing data from: ................{}'.format(year))\n",
    "        df.sort_values(by =['DATE_TIME'],inplace=True)\n",
    "        dt_used = False\n",
    "        df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])\n",
    "        for building in df.columns:\n",
    "            if(not dt_used):\n",
    "                dt_used = True\n",
    "            else:\n",
    "                building_name = mit_name_clean(building)\n",
    "                print('Building.....{}'.format(building_name))\n",
    "                subset = df[['DATE_TIME',building]]\n",
    "                subset = subset[subset.apply(lambda x: str(x[building]) not in ERROR_VALUES,axis=1)]\n",
    "                if len(subset) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    subset[building] = subset.apply(lambda x: float(x[building]), axis = 1)\n",
    "                    cleaned = subset[subset[building]!=0]\n",
    "                    dset_name ='{}/{}'.format(building_name,year)\n",
    "                    grp[dset_name] = mit_aggregate(cleaned,building)\n",
    "                \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Metadata Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdata_path = '{}/{}'.format(home,mdata)\n",
    "df = pd.read_csv(mdata_path)\n",
    "with h5py.File(file_loc) as f: \n",
    "    ds = f[home]\n",
    "    for build in ds.keys():\n",
    "        if(build in df['Building'].tolist()):\n",
    "            psu = df[df['Building']==build]['PSU'].tolist()[0]\n",
    "        building = ds[build]\n",
    "        building.attrs['PSU'] = psu\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color =#bf5700>Ireland Dataset</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = 'ireland'                        #Insert home directory where data is stored\n",
    "ext = '.txt'                            #Insert data storage type\n",
    "separ = ' '                             #Separation\n",
    "mdata = 'ireland/meta/metadata.csv'     #Point to location of metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ireland\\\\File1.txt',\n",
       " 'ireland\\\\File2.txt',\n",
       " 'ireland\\\\File3.txt',\n",
       " 'ireland\\\\File4.txt',\n",
       " 'ireland\\\\File5.txt',\n",
       " 'ireland\\\\File6.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root, folders,x = [m for m in os.walk(home)][0]\n",
    "data_files = [os.path.join(home,dfile) for dfile in x]\n",
    "data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ireland dataset is like the MIT dataset in that it too is shaped differently than our desires.\n",
    "<ul>\n",
    "<li>The files are segregated randomly, 1000 meters all years to a file instead of by building</li>\n",
    "<li>The list is NOT ordered, meaning that time intervals next to each other in the original scrape will NOT necessarily be together in the final </li>\n",
    "<li>In addition the separation between timestamps is not hourly as we would like but every 30 minutes </li>\n",
    "<li> Most curiously, there is an interesting format this dataset uses for date time, of 'xxxyy' where 'xxx' is days after Dec. 31, 2008 and yy is 30 minute intervals. 00:00 - 00:30 is yy=01\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Data Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ireland dataset stores date format in a weird form: \n",
    "def ireland_date(datum):\n",
    "    #First three digits are the # of days after 31 Dec 2008\n",
    "    dat = str(datum)\n",
    "    num_days = int(float(dat[:3]))\n",
    "    #Last three digits are the # of 30 minute intervals (1 = 0:0-0:30)\n",
    "    times = int(float(dat[3:]))\n",
    "    ORIGIN = dt.datetime(2008,12,31)\n",
    "    DAY_INCREMENT = dt.timedelta(days=1)\n",
    "    MIN_INCREMENT = dt.timedelta(minutes = 30)\n",
    "    return ORIGIN + DAY_INCREMENT*num_days + MIN_INCREMENT*(times-1)\n",
    "def aggregate(sorted_array):\n",
    "    sorted_array.drop_duplicates(subset = 1, keep = 'first', inplace = True )\n",
    "    times = sorted_array[1].dt.to_pydatetime().tolist()\n",
    "    usage = sorted_array[2].tolist()\n",
    "    aggregated_times = []\n",
    "    aggregated_usage = []\n",
    "    sets = {}\n",
    "    current = 2009 \n",
    "    changed = False\n",
    "    for i in range(len(times)-1):\n",
    "        if(times[i].year==2010 and not changed):\n",
    "            changed = True\n",
    "            sets[2009]=np.hstack((np.array(aggregated_times).reshape((-1,1)),np.array(aggregated_usage).reshape((-1,1))))\n",
    "            aggregated_times = []\n",
    "            aggregated_usage = []\n",
    "        if(times[i].minute==0 and times[i+1]-times[i] ==dt.timedelta(minutes=30)):\n",
    "            aggregated_times.append(times[i].strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8'))\n",
    "            aggregated_usage.append(str(usage[i]+usage[i+1]).encode('utf8'))\n",
    "    sets[2010] = np.hstack((np.array(aggregated_times).reshape((-1,1)),np.array(aggregated_usage).reshape((-1,1))))\n",
    "    return sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be very careful with running the script - it can take up to 2 hours!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_loc) as f:\n",
    "    counter = 0 \n",
    "    for dfile in data_files:\n",
    "        print('Now transcribing Data File: {}'.format(dfile))\n",
    "        data = pd.read_csv(dfile,sep=separ,header=None)\n",
    "        buildings = data[0].unique()\n",
    "        for b in buildings:\n",
    "            counter = counter + 1\n",
    "            print('-----Building #{}:.........ID#:{}'.format(counter,b))\n",
    "            subset = data[data[0]==b]\n",
    "            subset[1] = subset.apply(lambda x: ireland_date(x[1]),axis = 1)\n",
    "            subset.sort(1,inplace = True)\n",
    "            sets = aggregate(subset)\n",
    "            for key in sets.keys():\n",
    "                f['ireland/{}/{}'.format(b,key)]=sets[key]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Metadata Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ireland_meta():\n",
    "    TIMEZONE = \"Europe/Cork\"\n",
    "    INDUSTRY = \"Residential\"\n",
    "    metadata = pd.read_csv('IRELAND/meta/metadata.csv',encoding = 'ISO-8859-1',header=None)\n",
    "    subset = metadata.loc[1:,[0,34,38,39]]\n",
    "    with h5py.File(file_loc) as f:\n",
    "        subset[0] = [str(x) for x in subset[0]]\n",
    "        length = len(subset)\n",
    "        i=0\n",
    "        dset = f['ireland']\n",
    "        for meterid in subset[0]:\n",
    "            i = i+1\n",
    "            print('Adding special metadata for Building #{}/{}............ID:{}'.format(i,length,meterid))\n",
    "            grp = dset[meterid] if meterid in dset else None\n",
    "            if grp == None:\n",
    "                print('ID not found?')\n",
    "            else:\n",
    "                house_type = int(subset[subset[0]==meterid][34])\n",
    "                if(house_type==1):\n",
    "                    grp.attrs['PSU'] = 'Apartment'\n",
    "                elif(house_type==2 or house_type==3 or house_type==4 or house_type==5):\n",
    "                    grp.attrs['PSU'] = 'House'\n",
    "                area = float(subset[subset[0]==meterid][38])\n",
    "                if(area != 999999999):\n",
    "                    sqft = area\n",
    "                    units = int(subset[subset[0]==meterid][39])\n",
    "                    if(units==1):\n",
    "                        sqft = area*10.7639    #m^2 to sqft\n",
    "                    grp.attrs['Sqft'] = sqft\n",
    "        for mid in f['ireland'].keys():\n",
    "            print(mid)\n",
    "            grp = f['ireland/{}'.format(mid)] \n",
    "            grp.attrs['Timezone'] = TIMEZONE\n",
    "            grp.attrs['Industry'] = INDUSTRY\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ireland_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color = #bf5700>Genome data set</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = 'genome'                         #Insert home directory where data is stored\n",
    "ext = '.csv'                            #Insert data storage type\n",
    "separ = ' '                             #Separation\n",
    "mdata = 'genome/meta_open.csv'          #Point to location of metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attrs(row):\n",
    "    name = row[0]\n",
    "    industry = row[5]\n",
    "    psu = row[9]\n",
    "    sqft = row[11]\n",
    "    subindustry = row[13]\n",
    "    timezone = row[14]\n",
    "    dataend = to_dt_md(row[1]).year\n",
    "    datastart = to_dt_md(row[2]).year\n",
    "    return {'Name':name,'Industry':industry,'PSU':psu,'Sqft':sqft,'Subindustry':subindustry,'Timezone':timezone,'End':dataend,'Start':datastart}\n",
    "def clean_nan(dataframe,column):\n",
    "    return dataframe[dataframe.apply(lambda x: not np.isnan(x[column]),axis=1)]\n",
    "def parse():\n",
    "    DATA_PATH = 'Genome/temp_open_utc.csv'\n",
    "    METADATA_PATH = 'Genome/meta_open.csv'\n",
    "    ds = pd.read_csv(DATA_PATH)\n",
    "    with open(METADATA_PATH) as metadata:\n",
    "        reader = csv.reader(metadata)\n",
    "        header  = True\n",
    "        with h5py.File(file_loc) as dfile:\n",
    "            f = dfile['genome'] if 'genome' in dfile else dfile.create_group('genome')\n",
    "            i = 0\n",
    "            for row in reader:\n",
    "                if(header):           #If header row has not been read\n",
    "                    header = False    #Don't do anything\n",
    "                else:\n",
    "                    i=i+1\n",
    "                    attrs = get_attrs(row)\n",
    "                    year_start = attrs['Start']\n",
    "                    year_end = attrs['End']\n",
    "                    name = attrs['Name']\n",
    "                    print('Processing Building: #{}/507....... ID: {}'.format(i,name))\n",
    "                    cleaned = clean_nan(dataframe=ds,column=name).loc[:,['timestamp',name]]\n",
    "                    cleaned['timestamp'] = pd.to_datetime(cleaned['timestamp'])\n",
    "                    grp = f.create_group(name) if name not in f else f[name]\n",
    "                    #Set attributes from dictionary\n",
    "                    for attribute in attrs.items():\n",
    "                        if(attribute[0]!='Start' and attribute[0] != 'End' and attribute[0]!='Name'):\n",
    "                            grp.attrs[attribute[0]] = attribute[1]\n",
    "                    for year in np.arange(year_start,year_end+1):\n",
    "                        start = dt.datetime(year=year,month=1,day=1,hour=0,minute=0,second=0)\n",
    "                        end = dt.datetime(year=year+1,month=1,day=1,hour=0,minute=0,second=0)\n",
    "                        mask = (cleaned['timestamp']>=start)&(cleaned['timestamp']<end)\n",
    "                        subset = cleaned.loc[mask]\n",
    "                        dates = np.array([x.strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8') for x in subset['timestamp']],dtype=np.string_).reshape((-1,1))\n",
    "                        usage =np.array( [float(use) for use in subset[name]], dtype = np.float64).reshape((-1,1))\n",
    "                        grp[str(year)]= np.hstack((dates,usage))\n",
    "def to_dt_md(string):\n",
    "    return dt.datetime.strptime(string,\"%d/%m/%y %H:%M\")\n",
    "def to_dt(string):\n",
    "    cuts = string.split('+')[0] #Cleave the time zone modification\n",
    "    return dt.datetime.strptime(cuts,\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Building: #1/507....... ID: PrimClass_Everett\n",
      "Processing Building: #2/507....... ID: UnivClass_Clifford\n",
      "Processing Building: #3/507....... ID: Office_Elizabeth\n",
      "Processing Building: #4/507....... ID: Office_Ellie\n",
      "Processing Building: #5/507....... ID: PrimClass_Elisabeth\n",
      "Processing Building: #6/507....... ID: Office_Cristina\n",
      "Processing Building: #7/507....... ID: PrimClass_Jolie\n",
      "Processing Building: #8/507....... ID: PrimClass_Jaylin\n",
      "Processing Building: #9/507....... ID: Office_Jesus\n",
      "Processing Building: #10/507....... ID: PrimClass_Esmeralda\n",
      "Processing Building: #11/507....... ID: PrimClass_Eoghan\n",
      "Processing Building: #12/507....... ID: PrimClass_Edwin\n",
      "Processing Building: #13/507....... ID: PrimClass_Eli\n",
      "Processing Building: #14/507....... ID: PrimClass_Ethel\n",
      "Processing Building: #15/507....... ID: PrimClass_Ernesto\n",
      "Processing Building: #16/507....... ID: PrimClass_Emanuela\n",
      "Processing Building: #17/507....... ID: PrimClass_Emilio\n",
      "Processing Building: #18/507....... ID: PrimClass_Eleanor\n",
      "Processing Building: #19/507....... ID: PrimClass_Ezekiel\n",
      "Processing Building: #20/507....... ID: PrimClass_Elliott\n",
      "Processing Building: #21/507....... ID: PrimClass_Ellen\n",
      "Processing Building: #22/507....... ID: PrimClass_Evie\n",
      "Processing Building: #23/507....... ID: PrimClass_Elijah\n",
      "Processing Building: #24/507....... ID: PrimClass_Ezra\n",
      "Processing Building: #25/507....... ID: PrimClass_Edmund\n",
      "Processing Building: #26/507....... ID: PrimClass_Eva\n",
      "Processing Building: #27/507....... ID: Office_Erik\n",
      "Processing Building: #28/507....... ID: PrimClass_Ebony\n",
      "Processing Building: #29/507....... ID: PrimClass_Ethan\n",
      "Processing Building: #30/507....... ID: PrimClass_Elmer\n",
      "Processing Building: #31/507....... ID: PrimClass_Ervin\n",
      "Processing Building: #32/507....... ID: PrimClass_Uma\n",
      "Processing Building: #33/507....... ID: UnivClass_Tamra\n",
      "Processing Building: #34/507....... ID: PrimClass_Ernest\n",
      "Processing Building: #35/507....... ID: PrimClass_Emily\n",
      "Processing Building: #36/507....... ID: Office_Evelyn\n",
      "Processing Building: #37/507....... ID: PrimClass_Jayla\n",
      "Processing Building: #38/507....... ID: Office_Emer\n",
      "Processing Building: #39/507....... ID: PrimClass_Janiya\n",
      "Processing Building: #40/507....... ID: PrimClass_Umar\n",
      "Processing Building: #41/507....... ID: Office_Elena\n",
      "Processing Building: #42/507....... ID: PrimClass_Janice\n",
      "Processing Building: #43/507....... ID: Office_Jett\n",
      "Processing Building: #44/507....... ID: UnivDorm_Una\n",
      "Processing Building: #45/507....... ID: UnivLab_Paul\n",
      "Processing Building: #46/507....... ID: Office_Jerry\n",
      "Processing Building: #47/507....... ID: Office_Eileen\n",
      "Processing Building: #48/507....... ID: PrimClass_Uriah\n",
      "Processing Building: #49/507....... ID: PrimClass_Ulysses\n",
      "Processing Building: #50/507....... ID: Office_Lesa\n",
      "Processing Building: #51/507....... ID: UnivDorm_Claudia\n",
      "Processing Building: #52/507....... ID: Office_Emerald\n",
      "Processing Building: #53/507....... ID: Office_Ellis\n",
      "Processing Building: #54/507....... ID: UnivClass_Tammy\n",
      "Processing Building: #55/507....... ID: PrimClass_Jaden\n",
      "Processing Building: #56/507....... ID: Office_Elliot\n",
      "Processing Building: #57/507....... ID: PrimClass_Jermaine\n",
      "Processing Building: #58/507....... ID: PrimClass_Josephine\n",
      "Processing Building: #59/507....... ID: Office_Eddie\n",
      "Processing Building: #60/507....... ID: Office_Jackie\n",
      "Processing Building: #61/507....... ID: UnivDorm_Carla\n",
      "Processing Building: #62/507....... ID: UnivClass_Camden\n",
      "Processing Building: #63/507....... ID: PrimClass_Javier\n",
      "Processing Building: #64/507....... ID: PrimClass_Jeanette\n",
      "Processing Building: #65/507....... ID: PrimClass_Julius\n",
      "Processing Building: #66/507....... ID: PrimClass_Jaylinn\n",
      "Processing Building: #67/507....... ID: PrimClass_Johanna\n",
      "Processing Building: #68/507....... ID: PrimClass_Jodie\n",
      "Processing Building: #69/507....... ID: PrimClass_Johnathan\n",
      "Processing Building: #70/507....... ID: PrimClass_Janis\n",
      "Processing Building: #71/507....... ID: PrimClass_Jamal\n",
      "Processing Building: #72/507....... ID: PrimClass_Jose\n",
      "Processing Building: #73/507....... ID: PrimClass_Julianna\n",
      "Processing Building: #74/507....... ID: PrimClass_Jasmine\n",
      "Processing Building: #75/507....... ID: PrimClass_Jazmine\n",
      "Processing Building: #76/507....... ID: PrimClass_Justin\n",
      "Processing Building: #77/507....... ID: Office_Marla\n",
      "Processing Building: #78/507....... ID: PrimClass_Jody\n",
      "Processing Building: #79/507....... ID: PrimClass_Julianne\n",
      "Processing Building: #80/507....... ID: PrimClass_Justice\n",
      "Processing Building: #81/507....... ID: UnivClass_Craig\n",
      "Processing Building: #82/507....... ID: PrimClass_Jean\n",
      "Processing Building: #83/507....... ID: PrimClass_Jenna\n",
      "Processing Building: #84/507....... ID: PrimClass_Judith\n",
      "Processing Building: #85/507....... ID: PrimClass_Jessie\n",
      "Processing Building: #86/507....... ID: PrimClass_Joselyn\n",
      "Processing Building: #87/507....... ID: UnivLab_Bethany\n",
      "Processing Building: #88/507....... ID: PrimClass_Judy\n",
      "Processing Building: #89/507....... ID: PrimClass_Jarrett\n",
      "Processing Building: #90/507....... ID: PrimClass_Jeannine\n",
      "Processing Building: #91/507....... ID: PrimClass_Jeff\n",
      "Processing Building: #92/507....... ID: PrimClass_Julian\n",
      "Processing Building: #93/507....... ID: PrimClass_Jeanine\n",
      "Processing Building: #94/507....... ID: UnivClass_Jadon\n",
      "Processing Building: #95/507....... ID: Office_Maryann\n",
      "Processing Building: #96/507....... ID: UnivClass_Boyd\n",
      "Processing Building: #97/507....... ID: PrimClass_Jerome\n",
      "Processing Building: #98/507....... ID: PrimClass_Jacqueline\n",
      "Processing Building: #99/507....... ID: PrimClass_Jill\n",
      "Processing Building: #100/507....... ID: PrimClass_Jim\n",
      "Processing Building: #101/507....... ID: Office_Myron\n",
      "Processing Building: #102/507....... ID: PrimClass_Janet\n",
      "Processing Building: #103/507....... ID: PrimClass_Janie\n",
      "Processing Building: #104/507....... ID: PrimClass_Jensen\n",
      "Processing Building: #105/507....... ID: PrimClass_Jeffrey\n",
      "Processing Building: #106/507....... ID: Office_Conrad\n",
      "Processing Building: #107/507....... ID: UnivDorm_Prince\n",
      "Processing Building: #108/507....... ID: PrimClass_Jennifer\n",
      "Processing Building: #109/507....... ID: UnivClass_Therese\n",
      "Processing Building: #110/507....... ID: PrimClass_Jesse\n",
      "Processing Building: #111/507....... ID: Office_Mick\n",
      "Processing Building: #112/507....... ID: PrimClass_Jaxson\n",
      "Processing Building: #113/507....... ID: PrimClass_Jimmie\n",
      "Processing Building: #114/507....... ID: Office_Jayden\n",
      "Processing Building: #115/507....... ID: UnivDorm_Chelsey\n",
      "Processing Building: #116/507....... ID: PrimClass_Jayda\n",
      "Processing Building: #117/507....... ID: UnivDorm_Christopher\n",
      "Processing Building: #118/507....... ID: PrimClass_Jocelyn\n",
      "Processing Building: #119/507....... ID: PrimClass_Jeffery\n",
      "Processing Building: #120/507....... ID: UnivDorm_Cheyenne\n",
      "Processing Building: #121/507....... ID: UnivDorm_Patti\n",
      "Processing Building: #122/507....... ID: PrimClass_Jayson\n",
      "Processing Building: #123/507....... ID: UnivLab_Crystal\n",
      "Processing Building: #124/507....... ID: UnivDorm_Jeannette\n",
      "Processing Building: #125/507....... ID: Office_Moises\n",
      "Processing Building: #126/507....... ID: PrimClass_Jennie\n",
      "Processing Building: #127/507....... ID: PrimClass_Jonathon\n",
      "Processing Building: #128/507....... ID: UnivDorm_Christi\n",
      "Processing Building: #129/507....... ID: PrimClass_Jaqueline\n",
      "Processing Building: #130/507....... ID: UnivClass_Amari\n",
      "Processing Building: #131/507....... ID: Office_Tod\n",
      "Processing Building: #132/507....... ID: PrimClass_Jeremy\n",
      "Processing Building: #133/507....... ID: PrimClass_Josue\n",
      "Processing Building: #134/507....... ID: UnivDorm_Marlene\n",
      "Processing Building: #135/507....... ID: UnivClass_Teri\n",
      "Processing Building: #136/507....... ID: PrimClass_Julio\n",
      "Processing Building: #137/507....... ID: UnivDorm_Cheri\n",
      "Processing Building: #138/507....... ID: UnivDorm_Lorraine\n",
      "Processing Building: #139/507....... ID: PrimClass_Jacquelyn\n",
      "Processing Building: #140/507....... ID: PrimClass_Juanita\n",
      "Processing Building: #141/507....... ID: PrimClass_Joel\n",
      "Processing Building: #142/507....... ID: Office_Marcus\n",
      "Processing Building: #143/507....... ID: UnivClass_Candy\n",
      "Processing Building: #144/507....... ID: PrimClass_Joey\n",
      "Processing Building: #145/507....... ID: UnivDorm_Celeste\n",
      "Processing Building: #146/507....... ID: UnivClass_Shawna\n",
      "Processing Building: #147/507....... ID: PrimClass_Joanna\n",
      "Processing Building: #148/507....... ID: UnivClass_Ariana\n",
      "Processing Building: #149/507....... ID: PrimClass_Jediah\n",
      "Processing Building: #150/507....... ID: Office_Bobbi\n",
      "Processing Building: #151/507....... ID: Office_Georgia\n",
      "Processing Building: #152/507....... ID: PrimClass_Janelle\n",
      "Processing Building: #153/507....... ID: Office_Skyler\n",
      "Processing Building: #154/507....... ID: Office_Marc\n",
      "Processing Building: #155/507....... ID: Office_Mary\n",
      "Processing Building: #156/507....... ID: UnivDorm_Payton\n",
      "Processing Building: #157/507....... ID: Office_Ayesha\n",
      "Processing Building: #158/507....... ID: PrimClass_Jane\n",
      "Processing Building: #159/507....... ID: PrimClass_Jon\n",
      "Processing Building: #160/507....... ID: UnivDorm_Patty\n",
      "Processing Building: #161/507....... ID: UnivDorm_Adan\n",
      "Processing Building: #162/507....... ID: Office_Monty\n",
      "Processing Building: #163/507....... ID: UnivLab_Clint\n",
      "Processing Building: #164/507....... ID: PrimClass_Jake\n",
      "Processing Building: #165/507....... ID: Office_Joan\n",
      "Processing Building: #166/507....... ID: UnivLab_Cesar\n",
      "Processing Building: #167/507....... ID: UnivDorm_Cian\n",
      "Processing Building: #168/507....... ID: PrimClass_Johnnie\n",
      "Processing Building: #169/507....... ID: Office_Marlon\n",
      "Processing Building: #170/507....... ID: UnivDorm_Curtis\n",
      "Processing Building: #171/507....... ID: UnivDorm_Leonard\n",
      "Processing Building: #172/507....... ID: Office_Javon\n",
      "Processing Building: #173/507....... ID: UnivClass_Pete\n",
      "Processing Building: #174/507....... ID: PrimClass_Johnathon\n",
      "Processing Building: #175/507....... ID: Office_Jeanne\n",
      "Processing Building: #176/507....... ID: PrimClass_Jamie\n",
      "Processing Building: #177/507....... ID: Office_Pamela\n",
      "Processing Building: #178/507....... ID: Office_Allyson\n",
      "Processing Building: #179/507....... ID: PrimClass_Jacob\n",
      "Processing Building: #180/507....... ID: Office_Amelie\n",
      "Processing Building: #181/507....... ID: UnivDorm_Patrice\n",
      "Processing Building: #182/507....... ID: UnivClass_Alexander\n",
      "Processing Building: #183/507....... ID: Office_Maximus\n",
      "Processing Building: #184/507....... ID: Office_Ava\n",
      "Processing Building: #185/507....... ID: Office_Michael\n",
      "Processing Building: #186/507....... ID: Office_Caleb\n",
      "Processing Building: #187/507....... ID: Office_Scottie\n",
      "Processing Building: #188/507....... ID: Office_Cecelia\n",
      "Processing Building: #189/507....... ID: Office_Nelson\n",
      "Processing Building: #190/507....... ID: PrimClass_Angela\n",
      "Processing Building: #191/507....... ID: Office_Abbey\n",
      "Processing Building: #192/507....... ID: UnivDorm_Candace\n",
      "Processing Building: #193/507....... ID: UnivDorm_Clayton\n",
      "Processing Building: #194/507....... ID: UnivDorm_Ahmad\n",
      "Processing Building: #195/507....... ID: UnivClass_Armando\n",
      "Processing Building: #196/507....... ID: UnivDorm_April\n",
      "Processing Building: #197/507....... ID: Office_John\n",
      "Processing Building: #198/507....... ID: Office_Moses\n",
      "Processing Building: #199/507....... ID: UnivDorm_Corey\n",
      "Processing Building: #200/507....... ID: Office_Martha\n",
      "Processing Building: #201/507....... ID: UnivClass_Anamaria\n",
      "Processing Building: #202/507....... ID: UnivDorm_Cooper\n",
      "Processing Building: #203/507....... ID: UnivClass_Andy\n",
      "Processing Building: #204/507....... ID: UnivClass_Archie\n",
      "Processing Building: #205/507....... ID: Office_Mercedes\n",
      "Processing Building: #206/507....... ID: UnivClass_Stuart\n",
      "Processing Building: #207/507....... ID: Office_Mat\n",
      "Processing Building: #208/507....... ID: Office_Clifton\n",
      "Processing Building: #209/507....... ID: UnivLab_Cindy\n",
      "Processing Building: #210/507....... ID: UnivLab_Neil\n",
      "Processing Building: #211/507....... ID: Office_Marilyn\n",
      "Processing Building: #212/507....... ID: UnivDorm_Malachi\n",
      "Processing Building: #213/507....... ID: UnivDorm_Mathew\n",
      "Processing Building: #214/507....... ID: UnivDorm_Ashleigh\n",
      "Processing Building: #215/507....... ID: Office_Malik\n",
      "Processing Building: #216/507....... ID: UnivClass_Sam\n",
      "Processing Building: #217/507....... ID: UnivClass_Alec\n",
      "Processing Building: #218/507....... ID: UnivClass_Alvin\n",
      "Processing Building: #219/507....... ID: Office_Alyson\n",
      "Processing Building: #220/507....... ID: Office_Benjamin\n",
      "Processing Building: #221/507....... ID: UnivDorm_Piper\n",
      "Processing Building: #222/507....... ID: UnivDorm_Colton\n",
      "Processing Building: #223/507....... ID: UnivDorm_Paola\n",
      "Processing Building: #224/507....... ID: UnivLab_Miles\n",
      "Processing Building: #225/507....... ID: UnivDorm_Lawrence\n",
      "Processing Building: #226/507....... ID: UnivClass_Sheila\n",
      "Processing Building: #227/507....... ID: UnivDorm_Alonzo\n",
      "Processing Building: #228/507....... ID: UnivClass_Beatrice\n",
      "Processing Building: #229/507....... ID: Office_Mohammed\n",
      "Processing Building: #230/507....... ID: UnivDorm_Chester\n",
      "Processing Building: #231/507....... ID: UnivLab_Tami\n",
      "Processing Building: #232/507....... ID: Office_Bianca\n",
      "Processing Building: #233/507....... ID: UnivDorm_Phillip\n",
      "Processing Building: #234/507....... ID: UnivDorm_Mitch\n",
      "Processing Building: #235/507....... ID: UnivDorm_Casey\n",
      "Processing Building: #236/507....... ID: UnivClass_Colette\n",
      "Processing Building: #237/507....... ID: Office_Benthe\n",
      "Processing Building: #238/507....... ID: Office_Gemma\n",
      "Processing Building: #239/507....... ID: UnivDorm_Leticia\n",
      "Processing Building: #240/507....... ID: Office_Micheal\n",
      "Processing Building: #241/507....... ID: UnivClass_Abraham\n",
      "Processing Building: #242/507....... ID: UnivDorm_Leslie\n",
      "Processing Building: #243/507....... ID: UnivDorm_Poppy\n",
      "Processing Building: #244/507....... ID: Office_Corbin\n",
      "Processing Building: #245/507....... ID: Office_Aubrey\n",
      "Processing Building: #246/507....... ID: Office_Autumn\n",
      "Processing Building: #247/507....... ID: Office_Abigail\n",
      "Processing Building: #248/507....... ID: Office_Mark\n",
      "Processing Building: #249/507....... ID: UnivDorm_Lysander\n",
      "Processing Building: #250/507....... ID: UnivClass_Christian\n",
      "Processing Building: #251/507....... ID: UnivClass_Alexandra\n",
      "Processing Building: #252/507....... ID: UnivLab_Tracie\n",
      "Processing Building: #253/507....... ID: UnivClass_Philip\n",
      "Processing Building: #254/507....... ID: UnivDorm_Antonio\n",
      "Processing Building: #255/507....... ID: UnivDorm_Malcolm\n",
      "Processing Building: #256/507....... ID: Office_Travis\n",
      "Processing Building: #257/507....... ID: Office_Lena\n",
      "Processing Building: #258/507....... ID: Office_Max\n",
      "Processing Building: #259/507....... ID: UnivClass_Allen\n",
      "Processing Building: #260/507....... ID: UnivDorm_Ciaran\n",
      "Processing Building: #261/507....... ID: UnivClass_Pandora\n",
      "Processing Building: #262/507....... ID: Office_Gustavo\n",
      "Processing Building: #263/507....... ID: UnivDorm_Alyssa\n",
      "Processing Building: #264/507....... ID: UnivClass_Axel\n",
      "Processing Building: #265/507....... ID: UnivDorm_Phoebe\n",
      "Processing Building: #266/507....... ID: Office_Penny\n",
      "Processing Building: #267/507....... ID: Office_Garrett\n",
      "Processing Building: #268/507....... ID: UnivLab_Terrie\n",
      "Processing Building: #269/507....... ID: UnivLab_Tracy\n",
      "Processing Building: #270/507....... ID: UnivClass_Phyllis\n",
      "Processing Building: #271/507....... ID: UnivDorm_Marquis\n",
      "Processing Building: #272/507....... ID: PrimClass_Jazmin\n",
      "Processing Building: #273/507....... ID: UnivClass_Seb\n",
      "Processing Building: #274/507....... ID: Office_Annika\n",
      "Processing Building: #275/507....... ID: Office_Shawnette\n",
      "Processing Building: #276/507....... ID: Office_Cody\n",
      "Processing Building: #277/507....... ID: PrimClass_Jaiden\n",
      "Processing Building: #278/507....... ID: Office_Charles\n",
      "Processing Building: #279/507....... ID: UnivDorm_Avery\n",
      "Processing Building: #280/507....... ID: UnivLab_Taylor\n",
      "Processing Building: #281/507....... ID: UnivClass_Carl\n",
      "Processing Building: #282/507....... ID: Office_Pam\n",
      "Processing Building: #283/507....... ID: UnivClass_Conor\n",
      "Processing Building: #284/507....... ID: UnivLab_Lilly\n",
      "Processing Building: #285/507....... ID: UnivDorm_Mauricio\n",
      "Processing Building: #286/507....... ID: UnivClass_Maddison\n",
      "Processing Building: #287/507....... ID: UnivClass_Annmarie\n",
      "Processing Building: #288/507....... ID: Office_Asher\n",
      "Processing Building: #289/507....... ID: UnivDorm_Petar\n",
      "Processing Building: #290/507....... ID: UnivClass_Stephen\n",
      "Processing Building: #291/507....... ID: UnivClass_Serenity\n",
      "Processing Building: #292/507....... ID: Office_Mada\n",
      "Processing Building: #293/507....... ID: Office_Lillian\n",
      "Processing Building: #294/507....... ID: Office_Al\n",
      "Processing Building: #295/507....... ID: UnivLab_Preston\n",
      "Processing Building: #296/507....... ID: Office_Clarissa\n",
      "Processing Building: #297/507....... ID: UnivClass_Nishka\n",
      "Processing Building: #298/507....... ID: Office_Andrea\n",
      "Processing Building: #299/507....... ID: UnivDorm_Mckenzie\n",
      "Processing Building: #300/507....... ID: UnivClass_Ayanna\n",
      "Processing Building: #301/507....... ID: UnivClass_Nickolas\n",
      "Processing Building: #302/507....... ID: UnivClass_Amya\n",
      "Processing Building: #303/507....... ID: UnivClass_Nathaniel\n",
      "Processing Building: #304/507....... ID: UnivDorm_Alka\n",
      "Processing Building: #305/507....... ID: Office_Pat\n",
      "Processing Building: #306/507....... ID: UnivClass_Charlie\n",
      "Processing Building: #307/507....... ID: UnivLab_Alexis\n",
      "Processing Building: #308/507....... ID: UnivClass_Alexandria\n",
      "Processing Building: #309/507....... ID: UnivClass_Annabella\n",
      "Processing Building: #310/507....... ID: UnivClass_Antoinette\n",
      "Processing Building: #311/507....... ID: UnivDorm_Cathal\n",
      "Processing Building: #312/507....... ID: Office_Marianne\n",
      "Processing Building: #313/507....... ID: UnivLab_Cedric\n",
      "Processing Building: #314/507....... ID: Office_Angelo\n",
      "Processing Building: #315/507....... ID: Office_Anastasia\n",
      "Processing Building: #316/507....... ID: Office_Marvin\n",
      "Processing Building: #317/507....... ID: Office_Glenda\n",
      "Processing Building: #318/507....... ID: UnivClass_Sylvia\n",
      "Processing Building: #319/507....... ID: UnivLab_Susan\n",
      "Processing Building: #320/507....... ID: UnivClass_Peter\n",
      "Processing Building: #321/507....... ID: UnivClass_Aoibhe\n",
      "Processing Building: #322/507....... ID: UnivClass_Ciara\n",
      "Processing Building: #323/507....... ID: Office_Brian\n",
      "Processing Building: #324/507....... ID: UnivLab_Beth\n",
      "Processing Building: #325/507....... ID: UnivClass_Anya\n",
      "Processing Building: #326/507....... ID: Office_Maya\n",
      "Processing Building: #327/507....... ID: Office_Gisselle\n",
      "Processing Building: #328/507....... ID: Office_Phebian\n",
      "Processing Building: #329/507....... ID: Office_Matthew\n",
      "Processing Building: #330/507....... ID: UnivClass_Bob\n",
      "Processing Building: #331/507....... ID: Office_Amelia\n",
      "Processing Building: #332/507....... ID: Office_Jan\n",
      "Processing Building: #333/507....... ID: UnivClass_Alicia\n",
      "Processing Building: #334/507....... ID: Office_Madeleine\n",
      "Processing Building: #335/507....... ID: Office_Angelina\n",
      "Processing Building: #336/507....... ID: Office_Jackson\n",
      "Processing Building: #337/507....... ID: Office_Melinda\n",
      "Processing Building: #338/507....... ID: Office_Catherine\n",
      "Processing Building: #339/507....... ID: UnivDorm_Carter\n",
      "Processing Building: #340/507....... ID: Office_Gloria\n",
      "Processing Building: #341/507....... ID: UnivClass_Alexus\n",
      "Processing Building: #342/507....... ID: UnivDorm_Leann\n",
      "Processing Building: #343/507....... ID: UnivClass_Anne\n",
      "Processing Building: #344/507....... ID: UnivDorm_Cornelius\n",
      "Processing Building: #345/507....... ID: Office_Sinead\n",
      "Processing Building: #346/507....... ID: Office_Megan\n",
      "Processing Building: #347/507....... ID: UnivClass_Alfredo\n",
      "Processing Building: #348/507....... ID: UnivClass_Conner\n",
      "Processing Building: #349/507....... ID: UnivClass_Nelly\n",
      "Processing Building: #350/507....... ID: UnivDorm_Alex\n",
      "Processing Building: #351/507....... ID: Office_Shari\n",
      "Processing Building: #352/507....... ID: Office_Madisyn\n",
      "Processing Building: #353/507....... ID: Office_Natasha\n",
      "Processing Building: #354/507....... ID: UnivClass_Calvin\n",
      "Processing Building: #355/507....... ID: Office_Jude\n",
      "Processing Building: #356/507....... ID: UnivLab_Aniya\n",
      "Processing Building: #357/507....... ID: UnivDorm_Alphonso\n",
      "Processing Building: #358/507....... ID: Office_Angelica\n",
      "Processing Building: #359/507....... ID: Office_Morgan\n",
      "Processing Building: #360/507....... ID: UnivLab_Adrian\n",
      "Processing Building: #361/507....... ID: UnivLab_Christy\n",
      "Processing Building: #362/507....... ID: Office_Marion\n",
      "Processing Building: #363/507....... ID: UnivLab_Christine\n",
      "Processing Building: #364/507....... ID: UnivDorm_Cara\n",
      "Processing Building: #365/507....... ID: UnivClass_Adrienne\n",
      "Processing Building: #366/507....... ID: UnivClass_Nicholas\n",
      "Processing Building: #367/507....... ID: UnivClass_Nayeli\n",
      "Processing Building: #368/507....... ID: UnivClass_Carolyn\n",
      "Processing Building: #369/507....... ID: Office_Joni\n",
      "Processing Building: #370/507....... ID: UnivLab_Madelyn\n",
      "Processing Building: #371/507....... ID: UnivLab_Arianna\n",
      "Processing Building: #372/507....... ID: UnivLab_Ciel\n",
      "Processing Building: #373/507....... ID: Office_Paige\n",
      "Processing Building: #374/507....... ID: Office_Martin\n",
      "Processing Building: #375/507....... ID: UnivClass_Clay\n",
      "Processing Building: #376/507....... ID: PrimClass_Angel\n",
      "Processing Building: #377/507....... ID: Office_Muhammad\n",
      "Processing Building: #378/507....... ID: UnivLab_Ali\n",
      "Processing Building: #379/507....... ID: Office_Gladys\n",
      "Processing Building: #380/507....... ID: Office_Stella\n",
      "Processing Building: #381/507....... ID: UnivLab_Brad\n",
      "Processing Building: #382/507....... ID: Office_Louise\n",
      "Processing Building: #383/507....... ID: UnivClass_Noreen\n",
      "Processing Building: #384/507....... ID: UnivLab_Mack\n",
      "Processing Building: #385/507....... ID: UnivLab_Mariana\n",
      "Processing Building: #386/507....... ID: Office_Guillermo\n",
      "Processing Building: #387/507....... ID: Office_Paulina\n",
      "Processing Building: #388/507....... ID: UnivLab_Cayden\n",
      "Processing Building: #389/507....... ID: Office_Gabriela\n",
      "Processing Building: #390/507....... ID: UnivDorm_Pedro\n",
      "Processing Building: #391/507....... ID: UnivLab_Albert\n",
      "Processing Building: #392/507....... ID: UnivLab_Alaina\n",
      "Processing Building: #393/507....... ID: UnivLab_Ana\n",
      "Processing Building: #394/507....... ID: UnivDorm_Constance\n",
      "Processing Building: #395/507....... ID: Office_Carolina\n",
      "Processing Building: #396/507....... ID: UnivDorm_Cathalina\n",
      "Processing Building: #397/507....... ID: Office_Noel\n",
      "Processing Building: #398/507....... ID: UnivClass_Anika\n",
      "Processing Building: #399/507....... ID: UnivLab_Clodagh\n",
      "Processing Building: #400/507....... ID: UnivLab_Annette\n",
      "Processing Building: #401/507....... ID: Office_Alannah\n",
      "Processing Building: #402/507....... ID: Office_Aliyah\n",
      "Processing Building: #403/507....... ID: Office_Curt\n",
      "Processing Building: #404/507....... ID: UnivDorm_Carey\n",
      "Processing Building: #405/507....... ID: UnivLab_Angie\n",
      "Processing Building: #406/507....... ID: UnivClass_Celia\n",
      "Processing Building: #407/507....... ID: Office_Glenn\n",
      "Processing Building: #408/507....... ID: UnivLab_Andre\n",
      "Processing Building: #409/507....... ID: Office_Terrell\n",
      "Processing Building: #410/507....... ID: UnivDorm_Laura\n",
      "Processing Building: #411/507....... ID: UnivLab_Mario\n",
      "Processing Building: #412/507....... ID: UnivClass_Aidan\n",
      "Processing Building: #413/507....... ID: UnivLab_Annabelle\n",
      "Processing Building: #414/507....... ID: Office_Leland\n",
      "Processing Building: #415/507....... ID: UnivClass_Brett\n",
      "Processing Building: #416/507....... ID: UnivLab_Caesar\n",
      "Processing Building: #417/507....... ID: Office_Mikayla\n",
      "Processing Building: #418/507....... ID: UnivClass_Stephanie\n",
      "Processing Building: #419/507....... ID: UnivLab_Marshall\n",
      "Processing Building: #420/507....... ID: Office_Ayden\n",
      "Processing Building: #421/507....... ID: UnivClass_Cathleen\n",
      "Processing Building: #422/507....... ID: UnivLab_Collin\n",
      "Processing Building: #423/507....... ID: UnivDorm_Camila\n",
      "Processing Building: #424/507....... ID: UnivClass_Alejandra\n",
      "Processing Building: #425/507....... ID: Office_Colby\n",
      "Processing Building: #426/507....... ID: Office_Carissa\n",
      "Processing Building: #427/507....... ID: UnivLab_Peggy\n",
      "Processing Building: #428/507....... ID: Office_Paula\n",
      "Processing Building: #429/507....... ID: UnivLab_Marie\n",
      "Processing Building: #430/507....... ID: UnivLab_Suzette\n",
      "Processing Building: #431/507....... ID: UnivLab_Alberto\n",
      "Processing Building: #432/507....... ID: UnivLab_Aurora\n",
      "Processing Building: #433/507....... ID: UnivLab_Lyle\n",
      "Processing Building: #434/507....... ID: UnivLab_Lauren\n",
      "Processing Building: #435/507....... ID: UnivLab_Ariel\n",
      "Processing Building: #436/507....... ID: Office_Ashanti\n",
      "Processing Building: #437/507....... ID: UnivClass_Caoimhe\n",
      "Processing Building: #438/507....... ID: UnivLab_Cam\n",
      "Processing Building: #439/507....... ID: Office_Garman\n",
      "Processing Building: #440/507....... ID: UnivLab_Phil\n",
      "Processing Building: #441/507....... ID: Office_Cora\n",
      "Processing Building: #442/507....... ID: UnivLab_Ashlee\n",
      "Processing Building: #443/507....... ID: UnivClass_Nash\n",
      "Processing Building: #444/507....... ID: UnivLab_Allan\n",
      "Processing Building: #445/507....... ID: UnivLab_Bert\n",
      "Processing Building: #446/507....... ID: UnivClass_Chandler\n",
      "Processing Building: #447/507....... ID: UnivLab_Callie\n",
      "Processing Building: #448/507....... ID: UnivLab_Caitlin\n",
      "Processing Building: #449/507....... ID: UnivLab_Ashlynn\n",
      "Processing Building: #450/507....... ID: Office_Mason\n",
      "Processing Building: #451/507....... ID: UnivLab_Alfonso\n",
      "Processing Building: #452/507....... ID: UnivDorm_Alyshialynn\n",
      "Processing Building: #453/507....... ID: UnivLab_Amaya\n",
      "Processing Building: #454/507....... ID: UnivLab_Carol\n",
      "Processing Building: #455/507....... ID: Office_Clinton\n",
      "Processing Building: #456/507....... ID: UnivLab_Ashton\n",
      "Processing Building: #457/507....... ID: UnivLab_Cory\n",
      "Processing Building: #458/507....... ID: UnivLab_Cheyanne\n",
      "Processing Building: #459/507....... ID: Office_Paulette\n",
      "Processing Building: #460/507....... ID: UnivLab_Amy\n",
      "Processing Building: #461/507....... ID: Office_Dawn\n",
      "Processing Building: #462/507....... ID: Office_Perla\n",
      "Processing Building: #463/507....... ID: Office_Precious\n",
      "Processing Building: #464/507....... ID: Office_Patricia\n",
      "Processing Building: #465/507....... ID: UnivLab_Carlton\n",
      "Processing Building: #466/507....... ID: UnivLab_Camryn\n",
      "Processing Building: #467/507....... ID: UnivLab_Margret\n",
      "Processing Building: #468/507....... ID: UnivLab_Cecil\n",
      "Processing Building: #469/507....... ID: Office_Pasquale\n",
      "Processing Building: #470/507....... ID: UnivLab_Levi\n",
      "Processing Building: #471/507....... ID: UnivLab_Patsy\n",
      "Processing Building: #472/507....... ID: Office_Bryon\n",
      "Processing Building: #473/507....... ID: UnivClass_Abby\n",
      "Processing Building: #474/507....... ID: UnivLab_Brenna\n",
      "Processing Building: #475/507....... ID: UnivLab_Audra\n",
      "Processing Building: #476/507....... ID: UnivLab_Lester\n",
      "Processing Building: #477/507....... ID: UnivClass_Caitlyn\n",
      "Processing Building: #478/507....... ID: UnivLab_Cristian\n",
      "Processing Building: #479/507....... ID: UnivLab_Carley\n",
      "Processing Building: #480/507....... ID: UnivLab_Alina\n",
      "Processing Building: #481/507....... ID: UnivLab_Allison\n",
      "Processing Building: #482/507....... ID: UnivLab_Peyton\n",
      "Processing Building: #483/507....... ID: Office_Marcia\n",
      "Processing Building: #484/507....... ID: UnivLab_Aine\n",
      "Processing Building: #485/507....... ID: UnivLab_Santiago\n",
      "Processing Building: #486/507....... ID: UnivLab_Anita\n",
      "Processing Building: #487/507....... ID: UnivLab_Parker\n",
      "Processing Building: #488/507....... ID: UnivLab_Lee\n",
      "Processing Building: #489/507....... ID: Office_Dorian\n",
      "Processing Building: #490/507....... ID: Office_Shelly\n",
      "Processing Building: #491/507....... ID: UnivLab_Alisa\n",
      "Processing Building: #492/507....... ID: UnivLab_Louie\n",
      "Processing Building: #493/507....... ID: UnivLab_Priscilla\n",
      "Processing Building: #494/507....... ID: UnivLab_Patrick\n",
      "Processing Building: #495/507....... ID: UnivDorm_Adriana\n",
      "Processing Building: #496/507....... ID: UnivLab_Carole\n",
      "Processing Building: #497/507....... ID: UnivLab_Paris\n",
      "Processing Building: #498/507....... ID: Office_Pauline\n",
      "Processing Building: #499/507....... ID: UnivLab_Dianna\n",
      "Processing Building: #500/507....... ID: UnivDorm_Cecilia\n",
      "Processing Building: #501/507....... ID: Office_Luann\n",
      "Processing Building: #502/507....... ID: Office_Milton\n",
      "Processing Building: #503/507....... ID: Office_Lane\n",
      "Processing Building: #504/507....... ID: Office_Cameron\n",
      "Processing Building: #505/507....... ID: UnivLab_Lea\n",
      "Processing Building: #506/507....... ID: UnivLab_Carlos\n",
      "Processing Building: #507/507....... ID: UnivLab_Aoife\n"
     ]
    }
   ],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color = #bf5700>Pecan St. data set</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2 as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_connection(uid,pwd):\n",
    "    HOST = '67.78.67.93'         #dataport.cloud\n",
    "    PORT = 5434\n",
    "    USER = uid   \n",
    "    PWD = pwd    \n",
    "    conn = None\n",
    "    try:\n",
    "        conn = ps.connect(dbname = \"postgres\", host = HOST, user = USER, password= PWD,port = PORT)\n",
    "    except ps.Error as e:\n",
    "        print (\"connection error\"+ e)\n",
    "    return conn\n",
    "def destroy_connection(conn):\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfclean(df,clean_column):\n",
    "    return df[df.apply(lambda x: x[clean_column] is not None,axis=1)]\n",
    "def get_years(date_list):\n",
    "    years = [date.year for date in date_list]\n",
    "    return np.unique(years)\n",
    "def get_building(data_id, conn):\n",
    "    STMT = \"SELECT localhour,use FROM university.electricity_egauge_hours WHERE dataid={}\".format(data_id)\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(STMT)\n",
    "        rows = cur.fetchall()\n",
    "        df = pd.DataFrame(rows)\n",
    "        return df\n",
    "    except:\n",
    "        print('Querying Error')\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ids():\n",
    "    uniques_100=[22, 26, 48, 54, 59, 68, 77, 86, 93, 94, 101, 114, 115, 121, 130, 135, 160, 171, 187, 203, 222, 232, 243, 252,\n",
    "    267, 275, 280, 297, 330, 347, 364, 370, 379, 410, 434, 436, 457, 470, 483, 484, 490, 491, 499, 503, 507, 508, 516, 527, 545,\n",
    "    547, 555, 573, 575, 580, 585, 604, 621, 624, 645, 661, 668, 698, 739, 744, 765, 772, 774, 781, 796, 821, 861, 871, 878, 890,\n",
    "    898, 900, 930, 936, 946, 954, 974, 980, 991, 994]\n",
    "    uniques_200=[1037, 1069, 1086, 1103,1105,1153, 1167, 1169, 1185, 1192, 1202, 1283, 1310, 1314, 1331, 1334, 1350, 1354, 1392,\n",
    "    1403, 1415, 1450, 1463, 1464, 1479, 1500, 1507, 1508, 1524, 1551, 1577, 1586, 1589, 1597, 1601, 1617, 1629, 1632, 1642,\n",
    "    1681, 1696, 1697, 1700, 1714, 1718, 1731, 1766, 1782, 1790, 1791, 1792, 1796, 1800, 1801, 1830, 1832, 1845, 1854, 1879,\n",
    "    1889, 1947, 1953, 1994]\n",
    "    uniques_300 = [2004, 2018, 2031, 2034, 2062, 2072, 2075, 2094, 2129, 2144, 2156, 2158, 2171, 2199, 2204, 2207, 2233, 2242,\n",
    "    2247, 2335, 2337, 2354, 2360, 2361, 2365, 2366, 2378, 2401, 2449, 2458, 2461, 2465, 2470, 2472, 2505, 2510, 2520, 2523, 2532,\n",
    "    2557, 2575, 2606, 2638, 2641, 2667, 2710, 2742, 2750, 2751, 2755, 2769, 2787, 2814, 2815, 2818, 2824, 2829, 2845, 2859, 2864,\n",
    "    2873, 2903, 2907, 2925, 2931, 2945, 2953, 2965, 2974, 2980, 2986, 2992, 2995]\n",
    "    uniques_400 = [3009, 3032, 3036, 3039, 3044, 3087, 3092, 3104, 3126, 3134, 3143, 3160, 3192, 3204, 3215, 3221, 3224, 3235,\n",
    "    3263, 3268, 3273, 3299, 3310, 3353, 3367, 3368, 3392, 3394, 3401, 3411, 3413, 3425, 3426, 3443, 3456, 3482, 3484, 3500,\n",
    "    3504, 3506, 3510, 3519, 3527, 3531, 3538, 3544, 3577, 3615, 3631, 3632, 3635, 3649, 3652, 3676, 3678, 3687, 3719, 3721,\n",
    "    3723, 3734, 3736, 3778, 3789, 3795, 3806, 3829, 3831, 3849, 3864, 3873, 3883, 3886, 3893, 3916, 3918, 3935, 3938, 3953,\n",
    "    3964, 3967, 3973]\n",
    "    uniques_500 = [4000, 4022, 4031, 4042, 4053, 4083, 4095, 4135, 4147, 4154, 4193, 4213, 4220, 4224, 4251, 4296, 4297, 4298,\n",
    "    4302, 4313, 4321, 4329, 4336, 4342, 4352, 4357, 4373, 4375, 4383, 4416, 4438, 4447, 4473, 4495, 4499, 4505, 4514, 4526,\n",
    "    4544, 4575, 4590, 4601, 4633, 4641, 4660, 4670, 4674, 4699, 4703, 4732, 4761, 4767, 4773, 4776, 4800, 4830, 4856, 4864,\n",
    "    4874, 4910, 4920, 4922, 4927, 4934, 4944, 4946, 4956, 4957, 4967, 4974, 4998]\n",
    "    uniques_600 = [5009, 5026, 5035, 5060, 5087, 5109, 5129, 5164, 5187, 5209, 5218, 5226, 5246, 5252, 5262, 5271, 5275, 5279,\n",
    "    5288, 5298, 5317, 5356, 5357, 5371, 5395, 5400, 5403, 5438, 5439, 5448, 5449, 5450, 5456, 5485, 5539, 5545, 5552, 5568,\n",
    "    5615, 5652, 5658, 5673, 5677, 5718, 5728, 5738, 5746, 5749, 5759, 5778, 5784, 5785, 5786, 5796, 5809, 5810, 5814, 5817,\n",
    "    5852, 5874, 5889, 5892, 5904, 5909, 5921, 5938, 5944, 5949, 5959, 5972, 5994]\n",
    "    uniques_700=[6012, 6061, 6063, 6072, 6078, 6083, 6101, 6108, 6121, 6125, 6139, 6148, 6165, 6174, 6191, 6248, 6264, 6266, 6268,\n",
    "    6286, 6324, 6334, 6348, 6377, 6378, 6412, 6418, 6423, 6429, 6460, 6497, 6498, 6500, 6536, 6545, 6547, 6578, 6593, 6614, 6636,\n",
    "    6643, 6673, 6688, 6689, 6691, 6692, 6730, 6799, 6800, 6826, 6836, 6871, 6887, 6888, 6910, 6911, 6941, 6956, 6960, 6979, 6990]\n",
    "    uniques_800 = [7001, 7013, 7016, 7017, 7024, 7030, 7036, 7057, 7062, 7108, 7114, 7117, 7122, 7166, 7208, 7240, 7276, 7287, 7319,\n",
    "    7361, 7390, 7408, 7409, 7429, 7436, 7468, 7491, 7504, 7510, 7512, 7527, 7531, 7536, 7541, 7549, 7560, 7585, 7587, 7597, 7617,\n",
    "    7627, 7638, 7639, 7641, 7680, 7693, 7703, 7719, 7731, 7739, 7741, 7764, 7767, 7769, 7787, 7788, 7792, 7793, 7794, 7800, 7818,\n",
    "    7850, 7863, 7866, 7875, 7881, 7893, 7900, 7901, 7940, 7951, 7965, 7973, 7982, 7984, 7989]\n",
    "    uniques_900 = [8029, 8031, 8034, 8046, 8047, 8059, 8061, 8071, 8079, 8084, 8086, 8092, 8117, 8121, 8122, 8142, 8155, 8156, 8163,\n",
    "    8183, 8188, 8197, 8198, 8201, 8218, 8236, 8243, 8273, 8282, 8292, 8317, 8328, 8342, 8368, 8386, 8395, 8419, 8467, 8555, 8565,\n",
    "    8574, 8589, 8597, 8600, 8622, 8626, 8645, 8669, 8729, 8730, 8733, 8736, 8741, 8767, 8807, 8829, 8847, 8848, 8852, 8857, 8862,\n",
    "    8872, 8886, 8890, 8942, 8956, 8961, 8967, 8986, 8995]\n",
    "    uniques_1000=[9001, 9019, 9036, 9052, 9085, 9121, 9134, 9139, 9141, 9142, 9156, 9160, 9165, 9182, 9195, 9201, 9206, 9213, 9215,\n",
    "    9233, 9235, 9237, 9248, 9251, 9277, 9278, 9295, 9333, 9340, 9341, 9343, 9356, 9370, 9434, 9451, 9462, 9484, 9488, 9498, 9499,\n",
    "    9509, 9548, 9555, 9578, 9585, 9605, 9609, 9610, 9612, 9613, 9624, 9631, 9642, 9643, 9647, 9654, 9670, 9674, 9688, 9701, 9729,\n",
    "    9737, 9745, 9766, 9771, 9773, 9775, 9776, 9803, 9818, 9830, 9836, 9846, 9875, 9912, 9915, 9919, 9921, 9922, 9923, 9926, 9929,\n",
    "    9931, 9932, 9933, 9934, 9935, 9936, 9937, 9938, 9939, 9942, 9958, 9971, 9981, 9982, 9983]\n",
    "    return uniques_100 + uniques_200 + uniques_300 + uniques_400 + uniques_500 + uniques_600 + uniques_700 + uniques_800 + uniques_900 + uniques_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be very careful! Code can run as fast as 30 mintues to slightly over 2 hours depends on internet connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = str(input('Please enter Username:'))\n",
    "pwd = str(input('Please enter Password:'))\n",
    "conn = create_connection(user,pwd)\n",
    "\n",
    "with h5py.File(file_loc) as f:\n",
    "    pecan = f['pecan'] if 'pecan' in f else f.create_group('pecan')\n",
    "    i = 0 \n",
    "    for building_id in get_ids():\n",
    "        i = i+1 \n",
    "        print('Building #{}/747 : ID #{}.....'.format(i,building_id))\n",
    "        df = get_building(building_id,conn)\n",
    "        if(df.empty):\n",
    "                print('Empty dataset')\n",
    "        else:\n",
    "            grp = None\n",
    "            if '{}'.format(building_id) not in pecan:\n",
    "                grp = pecan.create_group(str(building_id))\n",
    "            else:\n",
    "                grp = pecan['{}'.format(building_id)] \n",
    "            years = get_years(df[0].tolist())\n",
    "            for year in years:\n",
    "                print('...Adding Dataset - {}'.format(year))\n",
    "                subset = dfclean(df[df.apply(lambda x: x[0].year==year,axis=1)],1)\n",
    "                date_list = [date.strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8') for date in subset[0].tolist()]\n",
    "                usage = [float(x) for x in subset[1].tolist()]\n",
    "                dataset = np.hstack((np.array(date_list,dtype=np.string_).reshape((-1,1)),np.array(usage,dtype=np.float64).reshape((-1,1))))\n",
    "                grp[str(year)]=dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplifying_function_2011(x):\n",
    "    if(x[2]==None or x[3]==None or x[4]==None or x[5] ==None or x[6]==None or x[7]==None):\n",
    "        return None\n",
    "    elif(x[2]==0 and x[3]==0 and x[4]==0 and x[5]==0 and x[6]==0 and x[7]==0):\n",
    "        return 'Apartment'\n",
    "    else:\n",
    "        return 'House'\n",
    "def add_mdata():\n",
    "    TIMEZONE = \"America/Austin\"\n",
    "    INDUSTRY = \"Residential\"\n",
    "    STMT_2013 = 'SELECT dataid,\"Conditioned_Square_Footage__c\",\"Type_of_Home__c\" FROM university.audits_2013_main'\n",
    "    STMT_2011 = 'SELECT dataid,conditions_square_foot,type_of_home_single_family,type_of_home_duplex,type_of_home_triplex,type_of_home_four_plex,type_of_home_condo,type_of_home_town_home FROM university.audits_2011'\n",
    "    df_2013 = pd.DataFrame()\n",
    "    df_2011 = pd.DataFrame()\n",
    "    user = str(input('Please enter Username:'))\n",
    "    pwd = str(input('Please enter Password:'))\n",
    "    try:\n",
    "        conn = create_connection(user,pwd)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(STMT_2011)\n",
    "        rows = cur.fetchall()\n",
    "        destroy_connection(conn)\n",
    "        df_2011 = pd.DataFrame(rows)\n",
    "    except ps.Error:\n",
    "        print('Querying Error_2011')\n",
    "    try:\n",
    "        conn = create_connection(user,pwd)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(STMT_2013)\n",
    "        rows = cur.fetchall()\n",
    "        destroy_connection(conn)\n",
    "        df_2013 = pd.DataFrame(rows)\n",
    "    except ps.Rrror:\n",
    "        print('Querying Error_2013')\n",
    "    house = df_2011.apply(simplifying_function_2011,axis=1)\n",
    "    df_2011[2] = house\n",
    "    subset_2011 = df_2011.loc[:,0:2]\n",
    "    df_2013[2]=[x if x=='Apartment' else 'House'for x in df_2013[2].tolist()]\n",
    "    union = df_2011.merge(df_2013,how='outer',left_on=0,right_on=0)\n",
    "    lists = get_ids()\n",
    "    with h5py.File(file_loc) as f:\n",
    "        for index,row in union.iterrows():\n",
    "            bid = int(row[0])\n",
    "            print(bid)\n",
    "            if bid in lists:\n",
    "                grp = f['pecan/{}'.format(bid)]\n",
    "                if(pd.isnull(row['2_y']) and not pd.isnull(row['2_x'])):\n",
    "                    grp.attrs['PSU'] = row['2_x']\n",
    "                elif(not pd.isnull(row['2_y'])):\n",
    "                    grp.attrs['PSU']=row['2_y']\n",
    "                if(pd.isnull(row['1_y']) and not pd.isnull(row['1_x'])):\n",
    "                    grp.attrs['Sqft'] = float(row['1_x'])\n",
    "                elif(not pd.isnull(row['1_y'])):\n",
    "                    grp.attrs['Sqft']=float(row['1_y'])\n",
    "        for key in f.keys():\n",
    "            grp =f[key]\n",
    "            grp.attrs['Timezone'] = TIMEZONE\n",
    "            grp.attrs['Industry'] = INDUSTRY\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_mdata()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
