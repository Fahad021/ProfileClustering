{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import h5py \n",
    "import csv \n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_loc = 'iel.hdf5'                   #Location of the hdf5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color =#bf5700>UT Austin Dataset</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Data Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the data files is segregated by building and then again by year. For example, 2015 data for the Welch building is located in 'utexas/WEL/2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = 'utexas'                         #Insert home directory where data is stored\n",
    "ext = '.csv'                            #Insert data storage type\n",
    "separ = ','                             #Separation\n",
    "mdata = 'utexas/utexas_metadata.csv'    #Point to location of metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root, folders,x = [m for m in os.walk(home)][0]\n",
    "folder_paths = [os.path.join(home, subdir) for subdir in folders]\n",
    "subfiles = []\n",
    "for f in folder_paths: \n",
    "    r,a, data = [m for m in os.walk(f)][0]\n",
    "    for d in data: \n",
    "        if os.path.splitext(d)[1]==ext:\n",
    "            subfiles.append(os.path.join(r,d))\n",
    "hdf_dsets = [s.replace('\\\\','/').replace(ext,'') for s in subfiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Subfiles' should now have a list of the complete path of all data files\n",
    "Ut dataset involves various time formats. convert_any_time converts string formats to pd datetime while float formats (already in pd format) are left alone: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_any_dtime(x):\n",
    "    try:\n",
    "        return dt.datetime.strptime(x, \"%m/%d/%y %H:%M\")\n",
    "    except:\n",
    "        try:\n",
    "            return dt.datetime.strptime(x, \"%m/%d/%Y %H:%M\")\n",
    "        except:\n",
    "            return None\n",
    "def read_mdata(mdata_file):\n",
    "    return pd.read_csv(mdata_file,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_db = read_mdata(mdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that compiles the data into the file. Be warned, data processing may take over 20 minutes. For a sense of where in the process the computer is, remove the commented portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_loc,'a') as f:\n",
    "    #counter = 0\n",
    "    for data_file in subfiles: \n",
    "        #counter = counter+1\n",
    "        #print('Now reading dataset #{}......................{}'.format(counter,data_file))\n",
    "        d = pd.read_csv(data_file).dropna(axis=0,how='any',inplace=False)\n",
    "        dt_dates = [convert_any_dtime(x) if type(x)==str else x for x in d['DateTime'] ]\n",
    "        dates= np.array( [x.strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8') for x in dt_dates], dtype = np.string_).reshape((-1,1))\n",
    "        usage =np.array([x for x in d['Electrical ( kWh )']],dtype=np.float64).reshape((-1,1))\n",
    "        hdfs_dset = data_file.replace('\\\\','/').replace(ext,'')\n",
    "        f[hdfs_dset] = np.hstack((dates,usage))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Metadata Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_loc,'a') as f:\n",
    "    for building in meta_db.index:\n",
    "        psu,sqft = meta_db.loc[building]\n",
    "        if '{}/{}'.format(home,building) in f:\n",
    "            bldg_grp = f['{}/{}'.format(home,building)]\n",
    "            bldg_grp.attrs['PSU'] = psu\n",
    "            bldg_grp.attrs['Sqft'] = sqft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color =#bf5700>MIT Dataset</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = 'MIT'                            #Insert home directory where data is stored\n",
    "ext = '.csv'                            #Insert data storage type\n",
    "separ = ','                             #Separation\n",
    "mdata = '#'                             #Point to location of metadata file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Data Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2014.csv', '2015.csv', '2016.csv']\n"
     ]
    }
   ],
   "source": [
    "root, folders,files = [m for m in os.walk(home)][0]\n",
    "subfiles = []\n",
    "for d in files: \n",
    "    if os.path.splitext(d)[1]==ext:\n",
    "        subfiles.append(os.path.join(root,d))\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIT dataset is shaped differently than our desires in a few ways: \n",
    "<ul>\n",
    "<li>The files are segregated by year instead of by building</li>\n",
    "<li>The files contain multiple 'error' values, so we need to screen more than just N/A</li>\n",
    "<li>The list is NOT ordered, meaning that time intervals next to each other in the original scrape will NOT necessarily be together in the final </li>\n",
    "<li>In addition the separation between timestamps is not hourly as we would like but every 15 minutes</li>\n",
    "<li> To further complicate matters, the list increments are in kilowatts (kW), a unit of power, not a unit of energy!!!  Power is related to energy usage in that power is average energy usage over a period of time. A Watt is 1 Joule per Second, and so 1 <b>kilowatt hour</b> is our standard measurement of energy: as\n",
    "$$1\\text{ kWH} =\\frac{\\text{1 Joule}}{\\text{1 second}}\\times \\text{3600 seconds} = 3600\\text{ Joule}$$\n",
    "A 1 kW power usage for 15 minutes is therefore $\\frac{1}{4} \\times 3600 \\text{ Joule} = 900 \\text{Joule}$\n",
    "Thus to find the total power usage over 4 different 15 minute intervals, take the average of the power usage of each individual file \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mit_name_clean(colname):\n",
    "    colname = colname.replace('RealPower','')\n",
    "    return colname.replace('TFR','')\n",
    "def mit_aggregate(sorted_array,name):\n",
    "    sorted_array.drop_duplicates(subset = 'DATE_TIME', keep = 'first', inplace = True )\n",
    "    times = sorted_array['DATE_TIME'].tolist()\n",
    "    usage = sorted_array[name].tolist()\n",
    "    aggregated_times = []\n",
    "    aggregated_usage = []\n",
    "    for i in range(len(times)-3):\n",
    "        one = (times[i+1]-times[i] ==dt.timedelta(minutes=15))\n",
    "        two = (times[i+2]-times[i+1] ==dt.timedelta(minutes=15))\n",
    "        three = (times[i+3]-times[i+2] ==dt.timedelta(minutes=15))\n",
    "        if(times[i].minute==0 and one and two and three):\n",
    "            aggregated_times.append(times[i].strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8'))\n",
    "            aggregated_usage.append(str((usage[i]+usage[i+1]+usage[i+2]+usage[i+3])/4).encode('utf8'))\n",
    "    return np.hstack((np.array(aggregated_times).reshape((-1,1)),np.array(aggregated_usage).reshape((-1,1))))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can account by the error values by setting up a filter, and deal with the time issue by 1) sorting the dataframe before partitioning it and 2) writing the aggregate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ERROR_VALUES = ['No Data','I/O Timeout','Error','Pt Created','Configure']\n",
    "with h5py.File(file_loc) as f:\n",
    "    grp = f[home] if home in f else f.create_group(home)\n",
    "    for dfile in subfiles: \n",
    "        year = dfile.split('\\\\')[-1].split('.')[0]\n",
    "        df = pd.read_csv(dfile)\n",
    "        print('Now analyzing data from: ................{}'.format(year))\n",
    "        df.sort_values(by =['DATE_TIME'],inplace=True)\n",
    "        dt_used = False\n",
    "        df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])\n",
    "        for building in df.columns:\n",
    "            if(not dt_used):\n",
    "                dt_used = True\n",
    "            else:\n",
    "                building_name = mit_name_clean(building)\n",
    "                print('Building.....{}'.format(building_name))\n",
    "                subset = df[['DATE_TIME',building]]\n",
    "                subset = subset[subset.apply(lambda x: str(x[building]) not in ERROR_VALUES,axis=1)]\n",
    "                if len(subset) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    subset[building] = subset.apply(lambda x: float(x[building]), axis = 1)\n",
    "                    cleaned = subset[subset[building]!=0]\n",
    "                    dset_name ='{}/{}'.format(building_name,year)\n",
    "                    grp[dset_name] = mit_aggregate(cleaned,building)\n",
    "                \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Metadata Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color =#bf5700>Ireland Dataset</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = 'ireland'                        #Insert home directory where data is stored\n",
    "ext = '.txt'                            #Insert data storage type\n",
    "separ = ' '                             #Separation\n",
    "mdata = 'ireland/meta/metadata.csv'     #Point to location of metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ireland\\\\File1.txt',\n",
       " 'ireland\\\\File2.txt',\n",
       " 'ireland\\\\File3.txt',\n",
       " 'ireland\\\\File4.txt',\n",
       " 'ireland\\\\File5.txt',\n",
       " 'ireland\\\\File6.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root, folders,x = [m for m in os.walk(home)][0]\n",
    "data_files = [os.path.join(home,dfile) for dfile in x]\n",
    "data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ireland dataset is like the MIT dataset in that it too is shaped differently than our desires.\n",
    "<ul>\n",
    "<li>The files are segregated randomly, 1000 meters all years to a file instead of by building</li>\n",
    "<li>The list is NOT ordered, meaning that time intervals next to each other in the original scrape will NOT necessarily be together in the final </li>\n",
    "<li>In addition the separation between timestamps is not hourly as we would like but every 30 minutes </li>\n",
    "<li> Most curiously, there is an interesting format this dataset uses for date time, of 'xxxyy' where 'xxx' is days after Dec. 31, 2008 and yy is 30 minute intervals. 00:00 - 00:30 is yy=01\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Data Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ireland dataset stores date format in a weird form: \n",
    "def ireland_date(datum):\n",
    "    #First three digits are the # of days after 31 Dec 2008\n",
    "    dat = str(datum)\n",
    "    num_days = int(float(dat[:3]))\n",
    "    #Last three digits are the # of 30 minute intervals (1 = 0:0-0:30)\n",
    "    times = int(float(dat[3:]))\n",
    "    ORIGIN = dt.datetime(2008,12,31)\n",
    "    DAY_INCREMENT = dt.timedelta(days=1)\n",
    "    MIN_INCREMENT = dt.timedelta(minutes = 30)\n",
    "    return ORIGIN + DAY_INCREMENT*num_days + MIN_INCREMENT*(times-1)\n",
    "def aggregate(sorted_array):\n",
    "    sorted_array.drop_duplicates(subset = 1, keep = 'first', inplace = True )\n",
    "    times = sorted_array[1].dt.to_pydatetime().tolist()\n",
    "    usage = sorted_array[2].tolist()\n",
    "    aggregated_times = []\n",
    "    aggregated_usage = []\n",
    "    sets = {}\n",
    "    current = 2009 \n",
    "    changed = False\n",
    "    for i in range(len(times)-1):\n",
    "        if(times[i].year==2010 and not changed):\n",
    "            changed = True\n",
    "            sets[2009]=np.hstack((np.array(aggregated_times).reshape((-1,1)),np.array(aggregated_usage).reshape((-1,1))))\n",
    "            aggregated_times = []\n",
    "            aggregated_usage = []\n",
    "        if(times[i].minute==0 and times[i+1]-times[i] ==dt.timedelta(minutes=30)):\n",
    "            aggregated_times.append(times[i].strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8'))\n",
    "            aggregated_usage.append(str(usage[i]+usage[i+1]).encode('utf8'))\n",
    "    sets[2010] = np.hstack((np.array(aggregated_times).reshape((-1,1)),np.array(aggregated_usage).reshape((-1,1))))\n",
    "    return sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be very careful with running the script - it can take up to 2 hours!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_loc) as f:\n",
    "    counter = 0 \n",
    "    for dfile in data_files:\n",
    "        print('Now transcribing Data File: {}'.format(dfile))\n",
    "        data = pd.read_csv(dfile,sep=separ,header=None)\n",
    "        buildings = data[0].unique()\n",
    "        for b in buildings:\n",
    "            counter = counter + 1\n",
    "            print('-----Building #{}:.........ID#:{}'.format(counter,b))\n",
    "            subset = data[data[0]==b]\n",
    "            subset[1] = subset.apply(lambda x: ireland_date(x[1]),axis = 1)\n",
    "            subset.sort(1,inplace = True)\n",
    "            sets = aggregate(subset)\n",
    "            for key in sets.keys():\n",
    "                f['ireland/{}/{}'.format(b,key)]=sets[key]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color = #333f48>Metadata Transformation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ireland_meta():\n",
    "    TIMEZONE = \"Europe/Cork\"\n",
    "    INDUSTRY = \"Residential\"\n",
    "    metadata = pd.read_csv('IRELAND/meta/metadata.csv',encoding = 'ISO-8859-1',header=None)\n",
    "    subset = metadata.loc[1:,[0,34,38,39]]\n",
    "    with h5py.File(file_loc) as f:\n",
    "        subset[0] = [str(x) for x in subset[0]]\n",
    "        length = len(subset)\n",
    "        i=0\n",
    "        dset = f['ireland']\n",
    "        for meterid in subset[0]:\n",
    "            i = i+1\n",
    "            print('Adding special metadata for Building #{}/{}............ID:{}'.format(i,length,meterid))\n",
    "            grp = dset[meterid] if meterid in dset else None\n",
    "            if grp == None:\n",
    "                print('ID not found?')\n",
    "            else:\n",
    "                house_type = int(subset[subset[0]==meterid][34])\n",
    "                if(house_type==1):\n",
    "                    grp.attrs['PSU'] = 'Apartment'\n",
    "                elif(house_type==2 or house_type==3 or house_type==4 or house_type==5):\n",
    "                    grp.attrs['PSU'] = 'House'\n",
    "                area = float(subset[subset[0]==meterid][38])\n",
    "                if(area != 999999999):\n",
    "                    sqft = area\n",
    "                    units = int(subset[subset[0]==meterid][39])\n",
    "                    if(units==1):\n",
    "                        sqft = area*10.7639    #m^2 to sqft\n",
    "                    grp.attrs['Sqft'] = sqft\n",
    "        for mid in f['ireland'].keys():\n",
    "            print(mid)\n",
    "            grp = f['ireland/{}'.format(mid)] \n",
    "            grp.attrs['Timezone'] = TIMEZONE\n",
    "            grp.attrs['Industry'] = INDUSTRY\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ireland_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color = #bf5700>Genome data set</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = 'genome'                         #Insert home directory where data is stored\n",
    "ext = '.csv'                            #Insert data storage type\n",
    "separ = ' '                             #Separation\n",
    "mdata = 'genome/meta_open.csv'          #Point to location of metadata file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color = #bf5700>Pecan St. data set</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2 as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_connection(uid,pwd):\n",
    "    HOST = '67.78.67.93'         #dataport.cloud\n",
    "    PORT = 5434\n",
    "    USER = uid   \n",
    "    PWD = pwd    \n",
    "    conn = None\n",
    "    try:\n",
    "        conn = ps.connect(dbname = \"postgres\", host = HOST, user = USER, password= PWD,port = PORT)\n",
    "    except ps.Error as e:\n",
    "        print (\"connection error\"+ e)\n",
    "    return conn\n",
    "def destroy_connection(conn):\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfclean(df,clean_column):\n",
    "    return df[df.apply(lambda x: x[clean_column] is not None,axis=1)]\n",
    "def get_years(date_list):\n",
    "    years = [date.year for date in date_list]\n",
    "    return np.unique(years)\n",
    "def get_building(data_id, conn):\n",
    "    STMT = \"SELECT localhour,use FROM university.electricity_egauge_hours WHERE dataid={}\".format(data_id)\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(STMT)\n",
    "        rows = cur.fetchall()\n",
    "        df = pd.DataFrame(rows)\n",
    "        return df\n",
    "    except:\n",
    "        print('Querying Error')\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ids():\n",
    "    uniques_100=[22, 26, 48, 54, 59, 68, 77, 86, 93, 94, 101, 114, 115, 121, 130, 135, 160, 171, 187, 203, 222, 232, 243, 252,\n",
    "    267, 275, 280, 297, 330, 347, 364, 370, 379, 410, 434, 436, 457, 470, 483, 484, 490, 491, 499, 503, 507, 508, 516, 527, 545,\n",
    "    547, 555, 573, 575, 580, 585, 604, 621, 624, 645, 661, 668, 698, 739, 744, 765, 772, 774, 781, 796, 821, 861, 871, 878, 890,\n",
    "    898, 900, 930, 936, 946, 954, 974, 980, 991, 994]\n",
    "    uniques_200=[1037, 1069, 1086, 1103,1105,1153, 1167, 1169, 1185, 1192, 1202, 1283, 1310, 1314, 1331, 1334, 1350, 1354, 1392,\n",
    "    1403, 1415, 1450, 1463, 1464, 1479, 1500, 1507, 1508, 1524, 1551, 1577, 1586, 1589, 1597, 1601, 1617, 1629, 1632, 1642,\n",
    "    1681, 1696, 1697, 1700, 1714, 1718, 1731, 1766, 1782, 1790, 1791, 1792, 1796, 1800, 1801, 1830, 1832, 1845, 1854, 1879,\n",
    "    1889, 1947, 1953, 1994]\n",
    "    uniques_300 = [2004, 2018, 2031, 2034, 2062, 2072, 2075, 2094, 2129, 2144, 2156, 2158, 2171, 2199, 2204, 2207, 2233, 2242,\n",
    "    2247, 2335, 2337, 2354, 2360, 2361, 2365, 2366, 2378, 2401, 2449, 2458, 2461, 2465, 2470, 2472, 2505, 2510, 2520, 2523, 2532,\n",
    "    2557, 2575, 2606, 2638, 2641, 2667, 2710, 2742, 2750, 2751, 2755, 2769, 2787, 2814, 2815, 2818, 2824, 2829, 2845, 2859, 2864,\n",
    "    2873, 2903, 2907, 2925, 2931, 2945, 2953, 2965, 2974, 2980, 2986, 2992, 2995]\n",
    "    uniques_400 = [3009, 3032, 3036, 3039, 3044, 3087, 3092, 3104, 3126, 3134, 3143, 3160, 3192, 3204, 3215, 3221, 3224, 3235,\n",
    "    3263, 3268, 3273, 3299, 3310, 3353, 3367, 3368, 3392, 3394, 3401, 3411, 3413, 3425, 3426, 3443, 3456, 3482, 3484, 3500,\n",
    "    3504, 3506, 3510, 3519, 3527, 3531, 3538, 3544, 3577, 3615, 3631, 3632, 3635, 3649, 3652, 3676, 3678, 3687, 3719, 3721,\n",
    "    3723, 3734, 3736, 3778, 3789, 3795, 3806, 3829, 3831, 3849, 3864, 3873, 3883, 3886, 3893, 3916, 3918, 3935, 3938, 3953,\n",
    "    3964, 3967, 3973]\n",
    "    uniques_500 = [4000, 4022, 4031, 4042, 4053, 4083, 4095, 4135, 4147, 4154, 4193, 4213, 4220, 4224, 4251, 4296, 4297, 4298,\n",
    "    4302, 4313, 4321, 4329, 4336, 4342, 4352, 4357, 4373, 4375, 4383, 4416, 4438, 4447, 4473, 4495, 4499, 4505, 4514, 4526,\n",
    "    4544, 4575, 4590, 4601, 4633, 4641, 4660, 4670, 4674, 4699, 4703, 4732, 4761, 4767, 4773, 4776, 4800, 4830, 4856, 4864,\n",
    "    4874, 4910, 4920, 4922, 4927, 4934, 4944, 4946, 4956, 4957, 4967, 4974, 4998]\n",
    "    uniques_600 = [5009, 5026, 5035, 5060, 5087, 5109, 5129, 5164, 5187, 5209, 5218, 5226, 5246, 5252, 5262, 5271, 5275, 5279,\n",
    "    5288, 5298, 5317, 5356, 5357, 5371, 5395, 5400, 5403, 5438, 5439, 5448, 5449, 5450, 5456, 5485, 5539, 5545, 5552, 5568,\n",
    "    5615, 5652, 5658, 5673, 5677, 5718, 5728, 5738, 5746, 5749, 5759, 5778, 5784, 5785, 5786, 5796, 5809, 5810, 5814, 5817,\n",
    "    5852, 5874, 5889, 5892, 5904, 5909, 5921, 5938, 5944, 5949, 5959, 5972, 5994]\n",
    "    uniques_700=[6012, 6061, 6063, 6072, 6078, 6083, 6101, 6108, 6121, 6125, 6139, 6148, 6165, 6174, 6191, 6248, 6264, 6266, 6268,\n",
    "    6286, 6324, 6334, 6348, 6377, 6378, 6412, 6418, 6423, 6429, 6460, 6497, 6498, 6500, 6536, 6545, 6547, 6578, 6593, 6614, 6636,\n",
    "    6643, 6673, 6688, 6689, 6691, 6692, 6730, 6799, 6800, 6826, 6836, 6871, 6887, 6888, 6910, 6911, 6941, 6956, 6960, 6979, 6990]\n",
    "    uniques_800 = [7001, 7013, 7016, 7017, 7024, 7030, 7036, 7057, 7062, 7108, 7114, 7117, 7122, 7166, 7208, 7240, 7276, 7287, 7319,\n",
    "    7361, 7390, 7408, 7409, 7429, 7436, 7468, 7491, 7504, 7510, 7512, 7527, 7531, 7536, 7541, 7549, 7560, 7585, 7587, 7597, 7617,\n",
    "    7627, 7638, 7639, 7641, 7680, 7693, 7703, 7719, 7731, 7739, 7741, 7764, 7767, 7769, 7787, 7788, 7792, 7793, 7794, 7800, 7818,\n",
    "    7850, 7863, 7866, 7875, 7881, 7893, 7900, 7901, 7940, 7951, 7965, 7973, 7982, 7984, 7989]\n",
    "    uniques_900 = [8029, 8031, 8034, 8046, 8047, 8059, 8061, 8071, 8079, 8084, 8086, 8092, 8117, 8121, 8122, 8142, 8155, 8156, 8163,\n",
    "    8183, 8188, 8197, 8198, 8201, 8218, 8236, 8243, 8273, 8282, 8292, 8317, 8328, 8342, 8368, 8386, 8395, 8419, 8467, 8555, 8565,\n",
    "    8574, 8589, 8597, 8600, 8622, 8626, 8645, 8669, 8729, 8730, 8733, 8736, 8741, 8767, 8807, 8829, 8847, 8848, 8852, 8857, 8862,\n",
    "    8872, 8886, 8890, 8942, 8956, 8961, 8967, 8986, 8995]\n",
    "    uniques_1000=[9001, 9019, 9036, 9052, 9085, 9121, 9134, 9139, 9141, 9142, 9156, 9160, 9165, 9182, 9195, 9201, 9206, 9213, 9215,\n",
    "    9233, 9235, 9237, 9248, 9251, 9277, 9278, 9295, 9333, 9340, 9341, 9343, 9356, 9370, 9434, 9451, 9462, 9484, 9488, 9498, 9499,\n",
    "    9509, 9548, 9555, 9578, 9585, 9605, 9609, 9610, 9612, 9613, 9624, 9631, 9642, 9643, 9647, 9654, 9670, 9674, 9688, 9701, 9729,\n",
    "    9737, 9745, 9766, 9771, 9773, 9775, 9776, 9803, 9818, 9830, 9836, 9846, 9875, 9912, 9915, 9919, 9921, 9922, 9923, 9926, 9929,\n",
    "    9931, 9932, 9933, 9934, 9935, 9936, 9937, 9938, 9939, 9942, 9958, 9971, 9981, 9982, 9983]\n",
    "    return uniques_100 + uniques_200 + uniques_300 + uniques_400 + uniques_500 + uniques_600 + uniques_700 + uniques_800 + uniques_900 + uniques_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be very careful! Code can run as fast as 30 mintues to slightly over 2 hours depends on internet connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = str(input('Please enter Username:'))\n",
    "pwd = str(input('Please enter Password:'))\n",
    "conn = create_connection(user,pwd)\n",
    "\n",
    "with h5py.File(file_loc) as f:\n",
    "    pecan = f['pecan'] if 'pecan' in f else f.create_group('pecan')\n",
    "    i = 0 \n",
    "    for building_id in get_ids():\n",
    "        i = i+1 \n",
    "        print('Building #{}/747 : ID #{}.....'.format(i,building_id))\n",
    "        df = get_building(building_id,conn)\n",
    "        if(df.empty):\n",
    "                print('Empty dataset')\n",
    "        else:\n",
    "            grp = None\n",
    "            if '{}'.format(building_id) not in pecan:\n",
    "                grp = pecan.create_group(str(building_id))\n",
    "            else:\n",
    "                grp = pecan['{}'.format(building_id)] \n",
    "            years = get_years(df[0].tolist())\n",
    "            for year in years:\n",
    "                print('...Adding Dataset - {}'.format(year))\n",
    "                subset = dfclean(df[df.apply(lambda x: x[0].year==year,axis=1)],1)\n",
    "                date_list = [date.strftime(\"%Y-%m-%d %H:%M:%S\").encode('utf8') for date in subset[0].tolist()]\n",
    "                usage = [float(x) for x in subset[1].tolist()]\n",
    "                dataset = np.hstack((np.array(date_list,dtype=np.string_).reshape((-1,1)),np.array(usage,dtype=np.float64).reshape((-1,1))))\n",
    "                grp[str(year)]=dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplifying_function_2011(x):\n",
    "    if(x[2]==None or x[3]==None or x[4]==None or x[5] ==None or x[6]==None or x[7]==None):\n",
    "        return None\n",
    "    elif(x[2]==0 and x[3]==0 and x[4]==0 and x[5]==0 and x[6]==0 and x[7]==0):\n",
    "        return 'Apartment'\n",
    "    else:\n",
    "        return 'House'\n",
    "def add_mdata():\n",
    "    TIMEZONE = \"America/Austin\"\n",
    "    INDUSTRY = \"Residential\"\n",
    "    STMT_2013 = 'SELECT dataid,\"Conditioned_Square_Footage__c\",\"Type_of_Home__c\" FROM university.audits_2013_main'\n",
    "    STMT_2011 = 'SELECT dataid,conditions_square_foot,type_of_home_single_family,type_of_home_duplex,type_of_home_triplex,type_of_home_four_plex,type_of_home_condo,type_of_home_town_home FROM university.audits_2011'\n",
    "    df_2013 = pd.DataFrame()\n",
    "    df_2011 = pd.DataFrame()\n",
    "    user = str(input('Please enter Username:'))\n",
    "    pwd = str(input('Please enter Password:'))\n",
    "    try:\n",
    "        conn = create_connection(user,pwd)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(STMT_2011)\n",
    "        rows = cur.fetchall()\n",
    "        destroy_connection(conn)\n",
    "        df_2011 = pd.DataFrame(rows)\n",
    "    except ps.Error:\n",
    "        print('Querying Error_2011')\n",
    "    try:\n",
    "        conn = create_connection(user,pwd)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(STMT_2013)\n",
    "        rows = cur.fetchall()\n",
    "        destroy_connection(conn)\n",
    "        df_2013 = pd.DataFrame(rows)\n",
    "    except ps.Rrror:\n",
    "        print('Querying Error_2013')\n",
    "    house = df_2011.apply(simplifying_function_2011,axis=1)\n",
    "    df_2011[2] = house\n",
    "    subset_2011 = df_2011.loc[:,0:2]\n",
    "    df_2013[2]=[x if x=='Apartment' else 'House'for x in df_2013[2].tolist()]\n",
    "    union = df_2011.merge(df_2013,how='outer',left_on=0,right_on=0)\n",
    "    lists = get_ids()\n",
    "    with h5py.File(file_loc) as f:\n",
    "        for index,row in union.iterrows():\n",
    "            bid = int(row[0])\n",
    "            print(bid)\n",
    "            if bid in lists:\n",
    "                grp = f['pecan/{}'.format(bid)]\n",
    "                if(pd.isnull(row['2_y']) and not pd.isnull(row['2_x'])):\n",
    "                    grp.attrs['PSU'] = row['2_x']\n",
    "                elif(not pd.isnull(row['2_y'])):\n",
    "                    grp.attrs['PSU']=row['2_y']\n",
    "                if(pd.isnull(row['1_y']) and not pd.isnull(row['1_x'])):\n",
    "                    grp.attrs['Sqft'] = float(row['1_x'])\n",
    "                elif(not pd.isnull(row['1_y'])):\n",
    "                    grp.attrs['Sqft']=float(row['1_y'])\n",
    "        for key in f.keys():\n",
    "            grp =f[key]\n",
    "            grp.attrs['Timezone'] = TIMEZONE\n",
    "            grp.attrs['Industry'] = INDUSTRY\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_mdata()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
